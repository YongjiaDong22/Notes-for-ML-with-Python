# 无监督学习-聚类

## 一、K-均值（K-means）聚类算法

### 1. 介绍

以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。

### 2. 步骤

+ 随机选择k个点作为**初始**的聚类中心
+ 对于剩下的点，根据其与聚类中心的距离，将其归入**最近**的簇
+ 对每个簇，计算所有点的**均值**作为新的聚类中心
+ 重复2和3，直到**聚类中心**不再发生改变

### 3. 图解

### 4. 应用（31个省份的消费性支出）

目的：了解1999年各个省份的消费水平在国内的情况。

思路：**sklearn.cluster.KMeans**

过程：

+ 建立工程，导入sklearn相关包
+ 加载数据，创建K-means算法实例，并进行训练，获得标签
+ 输出标签，查看结构

注意：

（1）调用K-Means方法所需参数

+ n_clusters：指定聚类中心的个数
+ init：初始聚类中心的初始化方法
+ max_iter：最大的迭代次数，即K-Means方法的最大迭代次数

***一般调用时只用给出n_clusters即可，init默认是k-means++，max_iter默认是300***

+ data：加载的数据
+ label：聚类后各数据所属的标签
+ fit_predict()：计算簇中心以及为簇分配序号

（2）自建loadData函数，用于加载数据

代码：

```
### 建立工程
#导入numpy
import numpy as np
#导入KMeans
from sklearn.cluster import KMeans

### loadData
def loadData(filePath):
    #以读写的方式打开文件
    fr = open(filePath,'r+',encoding='gbk')
    #一次读取整个文件
    lines = fr.readlines()
    #存储城市的各项消费信息
    retData = []
    #存储城市名称
    retCityName = []
    for line in lines:
        items = line.strip().split(",")
        retCityName.append(items[0])
        retData.append([float(items[i]) for i in range(1,len(items))])
    #返回值为retData和retCityName
    return retData,retCityName
 
if __name__ == '__main__':
### 加载数据
    #利用loadData方法读取数据
    data,cityName = loadData('city.txt')
    #创建实例，n_clusters指定聚类中心的个数
    km = KMeans(n_clusters=4)
    #调用Kmeans()和fit_predict()方法进行聚类计算
    label = km.fit_predict(data)
    expenses = np.sum(km.cluster_centers_,axis=1)
###展示结果
    CityCluster = [[],[],[],[]]
    #将城市按label分成设定的簇
    for i in range(len(cityName)):
        CityCluster[label[i]].append(cityName[i])
    #将每个簇的城市和消费输出
    for i in range(len(CityCluster)):
        #其中expense是聚类中心的数值加和，也就是平均消费水平
        print("Expenses:%.2f" % expenses[i])
        print(CityCluster[i])
```

其他知识点：

（1）Python的输出格式中，%d表示普通整数输出，如22；%4d表示宽度为4的整数输出，如0022；%.2f表示保留小数点后2位，如22.22。

（2）Sklearn的K-Means默认用的是**欧式距离**，没有设定计算距离方法的参数。如果想要自定义计算距离的方式，可以更改pairwise.py源代码，建议使用scipy.spatial.distance.cdist方法。

## 二、DBSCAN算法

### 1. 介绍

DBSCAN是一种基于密度的聚类算法，聚类是不需要预先指定簇的个数，最终的簇的个数不定。

DBSCAN算法将数据点分为三类：

+ 核心点：在半径Eps内含义超过MinPts数目的点
+ 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内
+ 噪声点：既不是核心点也不是边界点的点

### 2. 步骤

+ 将所有点标记为核心点、边界点或噪声点
+ 删除噪声点
+ 为距离在Eps之内的所有核心点之间赋予一条边
+ 每组连通的核心点形成一个簇
+ 将每个边界点指派到一个与之相关联的核心点的簇中（哪一个核心点的半径范围之内）

### 3. 图解

### 4. 应用（大学生上网模式）

目的：分析学生 **上网时间** 和 **上网时长** 的模式。

思路：**sklearn.cluster.DBSCAN**

过程：

+ 建立工程，导入sklearn相关包
+ 加载数据，预处理数据
+ 上网时间的聚类分析，创建DBSCAN算法实例，并进行训练，获得标签
+ 上网时长的聚类分析
+ 分析结果

注意：

（1）调用DBSCAN方法所需参数

+ eps：两个样本被看作邻居节点的最大距离
+ min_samples：簇的样本数
+ metric：距离计算方式

***示例：sklearn.cluster.DBSCAN(eps=0.5, min_samples=5, metric='euclidean')，表明邻居节点的最大距离为0.5，一个簇的样本数为5，距离计算采用欧式距离***

代码：

（1）时间聚类

```
### 建立工程
#导入numpy
import numpy as np
#导入sklearn.cluster
import sklearn.cluster as skc
#导入metrics
from sklearn import metrics
#导入matplotlib.pyplot
import matplotlib.pyplot as plt

### 加载数据
#词典存储MAC地址和上网信息
mac2id=dict()
#存储上网时长
onlinetimes=[]
#以读写的方式打开文件
f=open('TestData.txt',encoding='utf-8')
for line in f:
    #MAC地址
    mac=line.split(',')[2]
    #上网时长，取整数
    onlinetime=int(line.split(',')[6])
    #开始上网时间，取小时整数
    starttime=int(line.split(',')[4].split(' ')[1].split(':')[0])
    #mac2id词典，key是MAC地址，value是MAC地址对应的上网时长和开始上网时间
    if mac not in mac2id:
        mac2id[mac]=len(onlinetimes)
        onlinetimes.append((starttime,onlinetime))
    else:
        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]

#重新塑造数组，每行为一对startime和onlinetime
real_X=np.array(onlinetimes).reshape((-1,2))

### 上网时间聚类
#调用DBSCAN方法进行训练 
X=real_X[:,0:1]
#打印抽取的startime和onlinetime
print('x')
print(real_X)
db=skc.DBSCAN(eps=0.01,min_samples=20).fit(X)
labels = db.labels_

#打印数据被记上的标签
print('Labels:')
print(labels)
#计算标签为-1（噪音数据）的比例
ratio=len(labels[labels[:] == -1]) / len(labels)
print('Noise ratio:',format(ratio, '.2%'))

#计算并打印簇的个数，评价聚类效果 
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print('Estimated number of clusters: %d' % n_clusters_)
print("Silhouette Coefficient: %0.3f"% metrics.silhouette_score(X, labels))

#打印各个簇的标号和数据 
for i in range(n_clusters_):
    print('Cluster ',i,':')
    print(list(X[labels == i].flatten()))

### 转换直方图，分析结果     
plt.hist(X,24)
plt.show()
```

（2）时长聚类

```
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn import metrics
import matplotlib.pyplot as plt

mac2id = dict()
onlinetimes = []

f = open('TestData.txt',encoding='utf-8')
for line in f:
    mac = line.split(',')[2]
    onlinetime = int(line.split(',')[6])
    starttime = int(line.split(',')[4].split(' ')[1].split(':')[0])
    if mac not in mac2id:
        mac2id[mac] = len(onlinetimes)
        onlinetimes.append((starttime,onlinetime))
    else:
        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]

real_X=np.array(onlinetimes).reshape((-1,2))
X = np.log(1+real_X[:,1:])
db = DBSCAN(eps=0.14,min_samples=10).fit(X)
labels = db.labels_

print('Labels:')
print(labels)
ratio = len(labels[labels[:] == -1]) / len(labels)
print('Noise ratio:',format(ratio,'.2%'))

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print('Estimated number of clusters: %d' % n_clusters_)
print('Silhouette Coefficient: %0.3f' % metrics.silhouette_score(X,labels))

for i in range(n_clusters_):
    print('Cluster ',i,':')
    count = len(X[labels==i])
    mean = np.mean(real_X[labels==i][:,1])
    std = np.std(real_X[labels==i][:,1])
    print('\t number of sample: ',count)
    print('\t mean of sample: ',format(mean,'.1f'))
    print('\t std of sample: ',format(std,'.1f'))
```

其他知识点：

（1）对数变换：有利于对数据分布进行聚类分析。

（2）轮廓系数（Silhouette Coefficient）：是聚类效果好坏的一种评价方式。
